<!DOCTYPE html>
<html lang="zh-CN">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, minimum-scale=1.0, maximum-scale=2.0">
    <title>互联网网站的反爬虫策略浅析 - robbin的自言自语</title>
    <meta name="description" content="ruby, linux 因为搜索引擎的流行，网络爬虫已经成了很普及网络技术，除了专门做搜索的Google，Yahoo，微软，百度以外，几乎每个大型门户网站都有自己的搜索引擎，大大小小叫得出来名字得就几十种，还有各种不知名的几千几万种，对于一个内容型驱动的网站来说，受到网络爬虫的光顾是不可避免的。     一些智能的搜索引擎爬虫的爬取频率比较合理，对网站资源消耗比较少，但是很多糟糕的网络爬虫，对网页爬取能力很差，经常...">
    <meta name="author" content="Robbin Fan">
    <link href="/rss" rel="alternate" title="robbin的自言自语" type="application/rss+xml" />
    <link href="/stylesheets/default/document.css?1380098454" media="screen" rel="stylesheet" type="text/css" />
<link href="/stylesheets/default/content.css?1376392757" media="screen" rel="stylesheet" type="text/css" />
    <link href="/stylesheets/default/github.css?1376392757" media="screen" rel="stylesheet" type="text/css" />
<link href="/stylesheets/hightlight/github.min.css?1376388024" media="screen" rel="stylesheet" type="text/css" />

  </head>
  <body>
    <div id="flash-notice"></div>
    <div class="header">
      <div class="wrap">
      	<div class="user_img"><img src="/images/website.png" width="80" height="80" alt="website icon"></div>
        <div class="blog_title">robbin的自言自语</div>
        <div class="write">
          <a href="/login">登录</a>        </div>
        <div class="blog_motto">Small is beautiful, constraint is liberty.</div>
      </div>
    </div><!-- header -->
    
    <div class="nav">
      <div class="wrap clearfix">
      	<div class="search fr">
          <form action="/search" id="cse-search-box" method="get">
            <input type="hidden" name="cx" value="007540752216561218107:cgbwg20yn_o" />
            <input type="hidden" name="cof" value="FORID:10" />
            <input type="hidden" name="ie" value="UTF-8" />
            <input id="search-box" name="q" type="text" placeholder="Search" value="">
            <button id="search-button"></button>
          </form>
        </div>
        <ul>
          <li><a href="/" >首页</a></li>
          <li><a href="/blog" >博客</a></li>
          <li><a href="/note" class='current'>笔记</a></li>
          <li><a href="/tag" >分类</a></li>
          <li><a href="/weibo" >微博</a></li>
          <li><a href="http://robbinfan.com/blog/28/about-robbin" >关于</a></li>
        </ul>
      </div>
    </div><!-- nav -->
    
    <div class="wrap clearfix">
      

<div class="left">
  
  <div class="con">
    
    <div class="content">
    	<div class="tit clearfix">
  			<span class='note_icon' title='学习笔记'></span>
        <h1><a href="/blog/11/anti-crawler-strategy">互联网网站的反爬虫策略浅析</a></h1>
        <a class="markdown" title="Markdown格式原文" href="/blog/11/anti-crawler-strategy.md"></a>
      </div>
      <div class="info clearfix">
        <a class="tag" rel="tag" href="/tag/ruby"><span>ruby</span></a>
        <a class="tag" rel="tag" href="/tag/linux"><span>linux</span></a>
      </div>
      <div class="text">
        <p>因为搜索引擎的流行，网络爬虫已经成了很普及网络技术，除了专门做搜索的Google，Yahoo，微软，百度以外，几乎每个大型门户网站都有自己的搜索引擎，大大小小叫得出来名字得就几十种，还有各种不知名的几千几万种，对于一个内容型驱动的网站来说，受到网络爬虫的光顾是不可避免的。 </p>

<p>一些智能的搜索引擎爬虫的爬取频率比较合理，对网站资源消耗比较少，但是很多糟糕的网络爬虫，对网页爬取能力很差，经常并发几十上百个请求循环重复抓取，这种爬虫对中小型网站往往是毁灭性打击，特别是一些缺乏爬虫编写经验的程序员写出来的爬虫破坏力极强，造成的网站访问压力会非常大，会导致网站访问速度缓慢，甚至无法访问。 </p>

<h2>手工识别和拒绝爬虫的访问</h2>

<p>相当多的爬虫对网站会造成非常高的负载，因此识别爬虫的来源IP是很容易的事情。最简单的办法就是用netstat检查80端口的连接：</p>

<pre><code>netstat -nt | grep youhostip:80 | awk &#39;{print $5}&#39; | awk -F&quot;:&quot; &#39;{print $1}&#39;| sort | uniq -c | sort -r -n 
</code></pre>

<p>这行shell可以按照80端口连接数量对来源IP进行排序，这样可以直观的判断出来网页爬虫。一般来说爬虫的并发连接非常高。 </p>

<p>如果使用lighttpd做Web Server，那么就更简单了。lighttpd的mod_status提供了非常直观的并发连接的信息，包括每个连接的来源IP，访问的URL，连接状态和连接时间等信息，只要检查那些处于handle-request状态的高并发IP就可以很快确定爬虫的来源IP了。 </p>

<p>拒绝爬虫请求既可以通过内核防火墙来拒绝，也可以在web server拒绝，比方说用iptables拒绝： </p>

<pre><code>iptables -A INPUT -i eth0 -j DROP -p tcp --dport 80 -s 84.80.46.0/24  
</code></pre>

<p>直接封锁爬虫所在的C网段地址。这是因为一般爬虫都是运行在托管机房里面，可能在一个C段里面的多台服务器上面都有爬虫，而这个C段不可能是用户宽带上网，封锁C段可以很大程度上解决问题。 </p>

<h2>通过识别爬虫的User-Agent信息来拒绝爬虫</h2>

<p>有很多爬虫并不会以很高的并发连接爬取，一般不容易暴露自己；有些爬虫的来源IP分布很广，很难简单的通过封锁IP段地址来解决问题；另外还有很多各种各样的小爬虫，它们在尝试Google以外创新的搜索方式，每个爬虫每天爬取几万的网页，几十个爬虫加起来每天就能消耗掉上百万动态请求的资源，由于每个小爬虫单独的爬取量都很低，所以你很难把它从每天海量的访问IP地址当中把它准确的挖出来。 </p>

<p>这种情况下我们可以通过爬虫的User-Agent信息来识别。每个爬虫在爬取网页的时候，会声明自己的User-Agent信息，因此我们就可以通过记录和分析User-Agent信息来挖掘和封锁爬虫。我们需要记录每个请求的User-Agent信息，对于Rails来说我们可以简单的在app/controllers/application.rb里面添加一个全局的before_filter，来记录每个请求的User-Agent信息： </p>

<pre><code>logger.info &quot;HTTP_USER_AGENT #{request.env[&quot;HTTP_USER_AGENT&quot;]}&quot;  
</code></pre>

<p>然后统计每天的production.log，抽取User-Agent信息，找出访问量最大的那些User-Agent。要注意的是我们只关注那些爬虫的User-Agent信息，而不是真正浏览器User-Agent，所以还要排除掉浏览器User-Agent，要做到这一点仅仅需要一行shell：</p>

<pre><code>grep HTTP_USER_AGENT production.log | grep -v -E &#39;MSIE|Firefox|Chrome|Opera|Safari|Gecko&#39; | sort | uniq -c | sort -r -n | head -n 100 &gt; bot.log
</code></pre>

<p>统计结果类似这样：</p>

<pre><code>  57335 HTTP_USER_AGENT Baiduspider+(+http://www.baidu.com/search/spider.htm)
  56639 HTTP_USER_AGENT Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)
  42610 HTTP_USER_AGENT Mediapartners-Google
  19131 HTTP_USER_AGENT msnbot/2.0b (+http://search.msn.com/msnbot.htm)    
</code></pre>

<p>从日志就可以直观的看出每个爬虫的请求次数。要根据User-Agent信息来封锁爬虫是件很容易的事情，lighttpd配置如下：</p>

<pre><code>$HTTP[&quot;useragent&quot;] =~ &quot;qihoobot|^Java|Commons-HttpClient|Wget|^PHP|Ruby|Python&quot; {
  url.rewrite = ( &quot;^/(.*)&quot; =&gt; &quot;/crawler.html&quot; )
}
</code></pre>

<p>使用这种方式来封锁爬虫虽然简单但是非常有效，除了封锁特定的爬虫，还可以封锁常用的编程语言和HTTP类库的User-Agent信息，这样就可以避免很多无谓的程序员用来练手的爬虫程序对网站的骚扰。 </p>

<p>还有一种比较常见的情况，就是某个搜索引擎的爬虫对网站爬取频率过高，但是搜索引擎给网站带来了很多流量，我们并不希望简单的封锁爬虫，仅仅是希望降低爬虫的请求频率，减轻爬虫对网站造成的负载，那么我们可以这样做：</p>

<pre><code>$HTTP[&quot;user-agent&quot;] =~ &quot;Baiduspider+&quot; {
    connection.delay-seconds = 10
}    
</code></pre>

<p>对百度的爬虫请求延迟10秒钟再进行处理，这样就可以有效降低爬虫对网站的负载了。 </p>

<h2>通过网站流量统计系统和日志分析来识别爬虫</h2>

<p>有些爬虫喜欢修改User-Agent信息来伪装自己，把自己伪装成一个真实浏览器的User-Agent信息，让你无法有效的识别。这种情况下我们可以通过网站流量系统记录的真实用户访问IP来进行识别。 </p>

<p>主流的网站流量统计系统不外乎两种实现策略：一种策略是在网页里面嵌入一段js，这段js会向特定的统计服务器发送请求的方式记录访问量；另一种策略是直接分析服务器日志，来统计网站访问量。在理想的情况下，嵌入js的方式统计的网站流量应该高于分析服务器日志，这是因为用户浏览器会有缓存，不一定每次真实用户访问都会触发服务器的处理。但实际情况是，分析服务器日志得到的网站访问量远远高于嵌入js方式，极端情况下，甚至要高出10倍以上。 </p>

<p>现在很多网站喜欢采用awstats来分析服务器日志，来计算网站的访问量，但是当他们一旦采用Google Analytics来统计网站流量的时候，却发现GA统计的流量远远低于awstats，为什么GA和awstats统计会有这么大差异呢？罪魁祸首就是把自己伪装成浏览器的网络爬虫。这种情况下awstats无法有效的识别了，所以awstats的统计数据会虚高。 </p>

<p>其实作为一个网站来说，如果希望了解自己的网站真实访问量，希望精确了解网站每个频道的访问量和访问用户，应该用页面里面嵌入js的方式来开发自己的网站流量统计系统。自己做一个网站流量统计系统是件很简单的事情，写段服务器程序响应客户段js的请求，分析和识别请求然后写日志的同时做后台的异步统计就搞定了。 </p>

<p>通过流量统计系统得到的用户IP基本是真实的用户访问，因为一般情况下爬虫是无法执行网页里面的js代码片段的。所以我们可以拿流量统计系统记录的IP和服务器程序日志记录的IP地址进行比较，如果服务器日志里面某个IP发起了大量的请求，在流量统计系统里面却根本找不到，或者即使找得到，可访问量却只有寥寥几个，那么无疑就是一个网络爬虫。 </p>

<p>分析服务器日志统计访问最多的IP地址段一行shell就可以了： </p>

<pre><code>grep Processing production.log | awk &#39;{print $4}&#39; | awk -F&#39;.&#39; &#39;{print $1&quot;.&quot;$2&quot;.&quot;$3&quot;.0&quot;}&#39; | sort | uniq -c | sort -r -n | head -n 200 &gt; stat_ip.log
</code></pre>

<p>然后把统计结果和流量统计系统记录的IP地址进行对比，排除真实用户访问IP，再排除我们希望放行的网页爬虫，比方Google，百度，微软msn爬虫等等。最后的分析结果就就得到了爬虫的IP地址了。以下代码段是个简单的实现示意： </p>

<pre><code>whitelist = []
IO.foreach(&quot;#{RAILS_ROOT}/lib/whitelist.txt&quot;) { |line| whitelist &lt;&lt; line.split[0].strip if line }

realiplist = []
IO.foreach(&quot;#{RAILS_ROOT}/log/visit_ip.log&quot;) { |line|  realiplist &lt;&lt; line.strip if line }

iplist = []
IO.foreach(&quot;#{RAILS_ROOT}/log/stat_ip.log&quot;) do |line|
  ip = line.split[1].strip
  iplist &lt;&lt; ip if line.split[0].to_i &gt; 3000 &amp;&amp; !whitelist.include?(ip) &amp;&amp; !realiplist.include?(ip)
end 

Report.deliver_crawler(iplist)
</code></pre>

<p>分析服务器日志里面请求次数超过3000次的IP地址段，排除白名单地址和真实访问IP地址，最后得到的就是爬虫IP了，然后可以发送邮件通知管理员进行相应的处理。</p>

<h2>网站的实时反爬虫防火墙实现策略</h2>

<p>通过分析日志的方式来识别网页爬虫不是一个实时的反爬虫策略。如果一个爬虫非要针对你的网站进行处心积虑的爬取，那么他可能会采用分布式爬取策略，比方说寻找几百上千个国外的代理服务器疯狂的爬取你的网站，从而导致网站无法访问，那么你再分析日志是不可能及时解决问题的。所以必须采取实时反爬虫策略，要能够动态的实时识别和封锁爬虫的访问。 </p>

<p>要自己编写一个这样的实时反爬虫系统其实也很简单。比方说我们可以用memcached来做访问计数器，记录每个IP的访问频度，在单位时间之内，如果访问频率超过一个阀值，我们就认为这个IP很可能有问题，那么我们就可以返回一个验证码页面，要求用户填写验证码。如果是爬虫的话，当然不可能填写验证码，所以就被拒掉了，这样很简单就解决了爬虫问题。 </p>

<p>用memcache记录每个IP访问计数，单位时间内超过阀值就让用户填写验证码，用Rails编写的示例代码如下：</p>

<pre><code>ip_counter = Rails.cache.increment(request.remote_ip)
if !ip_counter
  Rails.cache.write(request.remote_ip, 1, :expires_in =&gt; 30.minutes)
elsif ip_counter &gt; 2000
  render :template =&gt; &#39;test&#39;, :status =&gt; 401 and return false
end
</code></pre>

<p>这段程序只是最简单的示例，实际的代码实现我们还会添加很多判断，比方说我们可能要排除白名单IP地址段，要允许特定的User-Agent通过，要针对登录用户和非登录用户，针对有无referer地址采取不同的阀值和计数加速器等等。 </p>

<p>此外如果分布式爬虫爬取频率过高的话，过期就允许爬虫再次访问还是会对服务器造成很大的压力，因此我们可以添加一条策略：针对要求用户填写验证码的IP地址，如果该IP地址短时间内继续不停的请求，则判断为爬虫，加入黑名单，后续请求全部拒绝掉。为此，示例代码可以改进一下： </p>

<pre><code>before_filter :ip_firewall, :except =&gt; :test
def ip_firewall
  render :file =&gt; &quot;#{RAILS_ROOT}/public/403.html&quot;, :status =&gt; 403 if BlackList.include?(ip_sec)
end    
</code></pre>

<p>我们可以定义一个全局的过滤器，对所有请求进行过滤，出现在黑名单的IP地址一律拒绝。对非黑名单的IP地址再进行计数和统计：</p>

<pre><code>ip_counter = Rails.cache.increment(request.remote_ip)
if !ip_counter
  Rails.cache.write(request.remote_ip, 1, :expires_in =&gt; 30.minutes)
elsif ip_counter &gt; 2000
  crawler_counter = Rails.cache.increment(&quot;crawler/#{request.remote_ip}&quot;)
  if !crawler_counter
    Rails.cache.write(&quot;crawler/#{request.remote_ip}&quot;, 1, :expires_in =&gt; 10.minutes)
  elsif crawler_counter &gt; 50
    BlackList.add(ip_sec)
    render :file =&gt; &quot;#{RAILS_ROOT}/public/403.html&quot;, :status =&gt; 403 and return false
  end
  render :template =&gt; &#39;test&#39;, :status =&gt; 401 and return false
end
</code></pre>

<p>如果某个IP地址单位时间内访问频率超过阀值，再增加一个计数器，跟踪他会不会立刻填写验证码，如果他不填写验证码，在短时间内还是高频率访问，就把这个IP地址段加入黑名单，除非用户填写验证码激活，否则所有请求全部拒绝。这样我们就可以通过在程序里面维护黑名单的方式来动态的跟踪爬虫的情况，甚至我们可以自己写个后台来手工管理黑名单列表，了解网站爬虫的情况。 </p>

<p>关于这个通用反爬虫的功能，我们开发一个开源的插件：<a href="https://github.com/csdn-dev/limiter">https://github.com/csdn-dev/limiter</a></p>

<p>这个策略已经比较智能了，但是还不够好！我们还可以继续改进： </p>

<p>1、用网站流量统计系统来改进实时反爬虫系统 </p>

<p>还记得吗？网站流量统计系统记录的IP地址是真实用户访问IP，所以我们在网站流量统计系统里面也去操作memcached，但是这次不是增加计数值，而是减少计数值。在网站流量统计系统里面每接收到一个IP请求，就相应的cache.decrement(key)。所以对于真实用户的IP来说，它的计数值总是加1然后就减1，不可能很高。这样我们就可以大大降低判断爬虫的阀值，可以更加快速准确的识别和拒绝掉爬虫。 </p>

<p>2、用时间窗口来改进实时反爬虫系统 </p>

<p>爬虫爬取网页的频率都是比较固定的，不像人去访问网页，中间的间隔时间比较无规则，所以我们可以给每个IP地址建立一个时间窗口，记录IP地址最近12次访问时间，每记录一次就滑动一次窗口，比较最近访问时间和当前时间，如果间隔时间很长判断不是爬虫，清除时间窗口，如果间隔不长，就回溯计算指定时间段的访问频率，如果访问频率超过阀值，就转向验证码页面让用户填写验证码。 </p>

<p>最终这个实时反爬虫系统就相当完善了，它可以很快的识别并且自动封锁爬虫的访问，保护网站的正常访问。不过有些爬虫可能相当狡猾，它也许会通过大量的爬虫测试来试探出来你的访问阀值，以低于阀值的爬取速度抓取你的网页，因此我们还需要辅助第3种办法，用日志来做后期的分析和识别，就算爬虫爬的再慢，它累计一天的爬取量也会超过你的阀值被你日志分析程序识别出来。 </p>

<p>总之我们综合运用上面的四种反爬虫策略，可以很大程度上缓解爬虫对网站造成的负面影响，保证网站的正常访问。    </p>

      </div>
      <div class="info clearfix">
        <div class="fr">
          <span class="author">robbin</span>
          <span class="time">2009-08-16发表</span>
          <span class="edit">2013-02-20更新</span>
          <span class="views" title="浏览次数">20797次浏览</span>
        </div>
      </div>
    </div><!-- content -->
  
    <div class="share clearfix">
      <span><script type="text/javascript">
(function(){
  var _w = 106 , _h = 58;
  var param = {
    url:location.href,
    type:'5',
    count:'1', /**是否显示分享数，1显示(可选)*/
    appkey:'2980914535', /**您申请的应用appkey,显示分享来源(可选)*/
    title:'互联网网站的反爬虫策略浅析:因为搜索引擎的流行，网络爬虫已经成了很普及网络技术，除了专门做搜索的Google，Yahoo，微软，百度以外，几乎每个大型门户网站都有自己的搜索引擎，大大小小叫得出来名字得就几十种，还有各种不知名的几千几万...', /**分享的文字内容(可选，默认为所在页面的title)*/
    pic:'', /**分享图片的路径(可选)*/
    ralateUid:'1654762921', /**关联用户的UID，分享微博会@该用户(可选)*/
    language:'zh_cn', /**设置语言，zh_cn|zh_tw(可选)*/
    rnd:new Date().valueOf()
  }
  var temp = [];
  for( var p in param ){
    temp.push(p + '=' + encodeURIComponent( param[p] || '' ) )
  }
  document.write('<iframe allowTransparency="true" frameborder="0" scrolling="no" src="http://hits.sinajs.cn/A1/weiboshare.html?' + temp.join('&') + '" width="'+ _w+'" height="'+_h+'"></iframe>')
})()
</script></span>
    </div><!-- share -->
    
    <div class="comment clearfix">
      <div id="comments">      
        <h2><span>6</span>条评论</h2>
        <ul>
          <li class="clearfix" id="664">
  <div class="user_img"><a target="_blank" rel="nofollow" href="http://weibo.com/u/2919215783"><img alt="河大老于" src="http://tp4.sinaimg.cn/2919215783/50/22819748690/1" /></a></div>
  <div class="cot_con">
 		<div class="info">
      <span class="user_name"><a target="_blank" rel="nofollow" href="http://weibo.com/u/2919215783">河大老于</a></span>
      <span class="time">2014-07-27</span>
    </div>
    <div class="comment_content"><p>学习了，一般直接过滤IP排序做的，看来还要学习python</p></div>
  </div>
</li>

<li class="clearfix" id="623">
  <div class="user_img"><a target="_blank" rel="nofollow" href="http://weibo.com/hit9"><img alt="Hit9" src="http://tp1.sinaimg.cn/2091896172/50/5659624501/1" /></a></div>
  <div class="cot_con">
 		<div class="info">
      <span class="user_name"><a target="_blank" rel="nofollow" href="http://weibo.com/hit9">Hit9</a></span>
      <span class="time">2014-03-06</span>
    </div>
    <div class="comment_content"><p>对于ip地址实时反爬虫的策略，如果某个内网都使用一个ip来访问呢。比如说校内网，某个大学的学生都使用一个ip的话，那么用这个策略来看这个ip就像一个爬虫，但实际上他们不是爬虫是主要用户。</p></div>
  </div>
</li>

<li class="clearfix" id="478">
  <div class="user_img"><a target="_blank" rel="nofollow" href="http://weibo.com/poplarddd"><img alt="poplarddd" src="http://tp3.sinaimg.cn/2692356934/50/5623342755/1" /></a></div>
  <div class="cot_con">
 		<div class="info">
      <span class="user_name"><a target="_blank" rel="nofollow" href="http://weibo.com/poplarddd">poplarddd</a></span>
      <span class="time">2013-09-19</span>
    </div>
    <div class="comment_content"><p>看了范大将军的博文，很是受用<br>
就是每看明白 “分析服务器日志统计访问最多的IP地址段一行shell”<br>
shell脚本中有这样的命令grep Processing，这个Processing是什么意思呢？</p></div>
  </div>
</li>

<li class="clearfix" id="226">
  <div class="user_img"><a target="_blank" rel="nofollow" href="http://weibo.com/samuraiwangheng"><img alt="王恒Frank" src="http://tp2.sinaimg.cn/1030546161/50/5627954999/1" /></a></div>
  <div class="cot_con">
 		<div class="info">
      <span class="user_name"><a target="_blank" rel="nofollow" href="http://weibo.com/samuraiwangheng">王恒Frank</a></span>
      <span class="time">2013-03-18</span>
    </div>
    <div class="comment_content"><p>嗯，我也想过，目前计算的是全部IP只计算数量的话是1024M（java int 2字节）<br>
不过还是想在一台机器上完成准确统计IP访问次数的统计<br>
想法是，按照IPv4的划分，第一阶段按照第一段地址划分，比如192.168.1.1就划归到192中去<br>
某个数字统计次数达到某个阀值，在将某个数字进入第二阶段统计，精确到第二段，如将其放入192-&gt;168中统计...如此一来可以节省部分内存，但开销依然很大。</p>

<blockquote>
<p>robbin 评论:<br>
可以用分布式的memcached或者redis。</p>

<blockquote>
<p>王恒Frank 评论:<br>
博主分享的反爬虫策略我看了两遍，相当完善，赞！<br>
博主的策略中使用了memcached计数，是否key就是IP地址呢？如果是IP地址，对于每天访问量较大（高并发，每日访问量在5000W+级以上）的网站是否会不适用呢？<br>
谢谢博主的分享，学习到很多东西。</p>
</blockquote>
</blockquote></div>
  </div>
</li>

<li class="clearfix" id="94">
  <div class="user_img"><a href="http://robbinfan.com"><img alt="robbin" src="/uploads/account/logo/1/logo.png" /></a></div>
  <div class="cot_con">
 		<div class="info">
      <span class="user_name"><a href="http://robbinfan.com">robbin</a></span>
      <span class="time">2013-02-27</span>
    </div>
    <div class="comment_content"><p>可以用分布式的memcached或者redis。</p>

<blockquote>
<p>王恒Frank 评论:<br>
博主分享的反爬虫策略我看了两遍，相当完善，赞！<br>
博主的策略中使用了memcached计数，是否key就是IP地址呢？如果是IP地址，对于每天访问量较大（高并发，每日访问量在5000W+级以上）的网站是否会不适用呢？<br>
谢谢博主的分享，学习到很多东西。</p>
</blockquote></div>
  </div>
</li>

<li class="clearfix" id="92">
  <div class="user_img"><a target="_blank" rel="nofollow" href="http://weibo.com/samuraiwangheng"><img alt="王恒Frank" src="http://tp2.sinaimg.cn/1030546161/50/5627954999/1" /></a></div>
  <div class="cot_con">
 		<div class="info">
      <span class="user_name"><a target="_blank" rel="nofollow" href="http://weibo.com/samuraiwangheng">王恒Frank</a></span>
      <span class="time">2013-02-27</span>
    </div>
    <div class="comment_content"><p>博主分享的反爬虫策略我看了两遍，相当完善，赞！<br>
博主的策略中使用了memcached计数，是否key就是IP地址呢？如果是IP地址，对于每天访问量较大（高并发，每日访问量在5000W+级以上）的网站是否会不适用呢？<br>
谢谢博主的分享，学习到很多东西。</p></div>
  </div>
</li>

        </ul>
      </div>
      <a name="comments"></a>
      <div class="relative">
        <div class="editor_switcher">
          <ul>
            <li class="current"><a id="publish_comment" href="#">发表评论</a></li>
            <li><a id="preview_comment" href="#">预览评论</a></li>
          </ul>
          <span style="float:right;">[<a href="http://wowubuntu.com/markdown/" target="_blank">评论可以使用Markdown格式</a>]</span>
        </div>
<form method="post" action="/blog/11/comments" protect_from_csrf="true" accept-charset="UTF-8" data-remote="true"><input value="cb0b4b551aca24e33d9cc893d399e5a52bff4dc2f88c0d3a256fa445088787ba" name="authenticity_token" type="hidden" />          <textarea class="comment" id="blog_comment_content" name="blog_comment[content]" rows="" cols=""></textarea>
          <div id="comment-error-info"></div>
          <div id="comment_preview" class="con comment"></div>
          <input class="comment_btn" value="" type="submit" />
</form>        <div class="box" style="">
          <button class="close"></button>
          <p>我想知道与谁交流思想，请用微博登录留言。</p>
          <button class="sina_btn"></button>
        </div><!-- box -->
      </div><!-- relative -->
      
    </div><!-- comment -->
     
  </div><!-- con -->
  
</div><!-- left -->

<div class="right">
  
<h3>肉饼铺子微信号</h3>
<div class="tag">
	<img alt="肉饼铺子微信订阅号" title="肉饼铺子微信订阅号" src="/images/robbin_foods_shop.jpg?1408504282" />
</div>

<h3>文章分类</h3>
<div class="tag">
  <a href="/tag/weixin">weixin<span>22</span></a>
  <a href="/tag/ruby">ruby<span>11</span></a>
  <a href="/tag/industry">industry<span>9</span></a>
  <a href="/tag/life">life<span>5</span></a>
  <a href="/tag/management">management<span>5</span></a>
  <a href="/tag/product">product<span>4</span></a>
  <a href="/tag/entrepreneur">entrepreneur<span>3</span></a>
  <a href="/tag/tech">tech<span>3</span></a>
  <a href="/tag/linux">linux<span>3</span></a>
</div>
  
<h3>热门文章</h3>
<div class="hot_blog">
  <a href="/blog/29/my-new-website">新的个人网站开张 robbinfan.com</a>
  <a href="/blog/31/why-love-shanghai">我为何喜欢上海不喜欢北京</a>
  <a href="/blog/38/orm-cache-sumup">Web应用的缓存设计模式</a>
  <a href="/blog/40/ruby-off-rails">Ruby社区应该去Rails化了</a>
  <a href="/blog/28/about-robbin">范凯的自我介绍</a>
</div>

<h3>最新评论</h3>
<ul class="comment">
	<li>
    <div>是啊，名字太复杂了。我女儿姓名是极度简化的：苏一...</div>
  	love_zi评论了<a href="/blog/73/alibaba#comments">阿里巴巴印象记 － 非常“传统”的互联网公司</a>
  </li>
	<li>
    <div>你现在选的儿子的名字，太复杂了（他可能要很大会写自己的名字），别人也很难念准确。请三思。

...</div>
  	大龙_168评论了<a href="/blog/73/alibaba#comments">阿里巴巴印象记 － 非常“传统”的互联网公司</a>
  </li>
	<li>
    <div>范克己，克己复礼为仁。...</div>
  	有时没事微博评论了<a href="/blog/72#comments">为儿子征求创意取名</a>
  </li>
	<li>
    <div>范俊祺...</div>
  	DevCrew评论了<a href="/blog/72#comments">为儿子征求创意取名</a>
  </li>
	<li>
    <div>小名就不能叫文文了 可以叫阿哲...</div>
  	Beviz评论了<a href="/blog/72#comments">为儿子征求创意取名</a>
  </li>
</ul>


<div class="rss clearfix">
  <a href="/rss">RSS订阅</a>
</div>

</div><!-- right -->


    </div><!-- content -->
    
    <div class="footer">
    	<a href="http://weibo.com/robbinfan" class="weibo">Weibo</a>
      <a href="https://github.com/robbin" class="github">Github</a>      
      <a href="mailto:fankai@gmail.com" class="mail">Email</a>
      <div class="powered-by">
        <a href="http://robbinfan.com" title="robbin的自言自语">Powered by robbin</a>&nbsp;
				<a href="http://www.elvenui.com/" title="Elvenui" target="_blank">Design by Elvenui</a>
      </div>
    </div><!-- footer -->
    <script src="/javascripts/jquery.js?1376388024" type="text/javascript"></script>
<script src="/javascripts/jquery-ujs.js?1376388024" type="text/javascript"></script>
<script src="/javascripts/application.js?1376388024" type="text/javascript"></script>
      <script src="/javascripts/highlight.min.js?1376388024" type="text/javascript"></script>
  <script type="text/javascript">
  $(function(){
    // hightlight code
    hljs.initHighlightingOnLoad();

    // response close button of weibo login div
    $('button.close').click(function() {
      $('div.box').hide();
    });

    // response sina login button
    $('button.sina_btn').click(function(){
      var width=Math.round((window.screen.width-800)/2);
      var height=Math.round((window.screen.height-600)/2);
      window.open('/weibo_login/?quick_login=1', 'weibo_third_part_authentication', 'height=300px, width=400px, top='+height+',left='+width+', scrollbars=no, resizable=no, status=no, resizable=no, menubar=no');
    });
    
    // validate empty comment
    $('input.comment_btn').click(function(){
      if ($('textarea#blog_comment_content').val().length < 3) {
        $('div#comment-error-info').html('评论太少，多写点').show().fadeOut(3000);
        return false;
      }
    });
    
    // preview comments
    $('a#publish_comment').click(function(){
      $('a#publish_comment').parent().addClass('current');
      $('a#preview_comment').parent().removeClass('current');
      $('div#comment_preview').hide();
      $('textarea#blog_comment_content').show();
      return false;
    });
    $('a#preview_comment').click(function(){
      $.post('/comment/preview', {term: $('textarea#blog_comment_content').val()}, function(data){
        $('a#preview_comment').parent().addClass('current');
        $('a#publish_comment').parent().removeClass('current');
        $('textarea#blog_comment_content').hide();
        $('div#comment_preview').html(data);
        $('div#comment_preview').show();
      });
      return false;
    });

    // quote comment
    $('div.cot_con a.quote_comment').click(function(){
      var commentBlock = $(this).closest('li');
      $.get('/comment/quote', {id: commentBlock.attr('id')}, function(data){
        var body = $('textarea#blog_comment_content').val() + data;
        $('textarea#blog_comment_content').val(body);
        $('textarea#blog_comment_content').focus();
      });
      return false;
    });
  });
  </script>

  </body>
</html>
